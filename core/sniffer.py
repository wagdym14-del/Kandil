import asyncio
import websockets
import json
import logging
import time
import streamlit as st  # Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ (Ù„Ø§ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±Ø¹Ø©)
from typing import Optional, List, Dict
from dataclasses import dataclass

# Ù†Ø¸Ø§Ù… ØªØ³Ø¬ÙŠÙ„ Ø¬Ù†Ø§Ø¦ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø¯Ù‚Ø©
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("SovereignSniffer.Ultra")

@dataclass
class MarketEvent:
    signature: str
    timestamp: float
    event_type: str
    risk_level: int
    raw_logs: List[str]

class PumpSniffer:
    """
    [2026-02-03] Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„ÙØ§Ø¦Ù‚ Ù„Ø±ØµØ¯ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© (Behavioral Gap Detector).
    Ù†Ø¸Ø§Ù… ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ (Parallel Stream Processing).
    ØªÙ… Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ø¹ Ù…ÙŠØ²Ø© Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ.
    """
    PROGRAM_ID = "6EF8rrecthR5DkZJbdz4P8hHKXY6yizQ2EtJhEqNpump"

    def __init__(self, wss_url: str = None, archiver=None, workers: int = 5):
        # Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: ÙŠØ³Ø­Ø¨ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ø®Ø²Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡ØŒ Ù…Ù…Ø§ ÙŠØ¶Ù…Ù† Ø§Ù„Ø¹Ù…Ù„ Ø³Ø­Ø§Ø¨ÙŠØ§Ù‹ ÙˆÙ…Ø­Ù„ÙŠØ§Ù‹
        self.wss_url = wss_url or st.secrets.get("WSS_URL_PRIMARY")
        self.archiver = archiver
        self.workers_count = workers
        self._queue = asyncio.Queue(maxsize=10000) # Ø·Ø§Ø¨ÙˆØ± Ø¶Ø®Ù… Ù„Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
        self.is_running = False
        self._performance_metrics = {"total_processed": 0, "dropped": 0}

    async def _subscribe(self, ws):
        """ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„"""
        subscribe_msg = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "logsSubscribe",
            "params": [
                {"mentions": [self.PROGRAM_ID]},
                {"commitment": "processed"}
            ]
        }
        await ws.send(json.dumps(subscribe_msg))
        logger.info(f"ðŸ“¡ [CONNECTED] Monitoring: {self.PROGRAM_ID[:8]}...")

    async def start_sniffing(self):
        """Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø±Ø§Ø¯Ø§Ø± Ø¨Ù†Ø¸Ø§Ù… Ø®ÙˆØ§Ø¯Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© (Worker Pool)"""
        if not self.wss_url:
            logger.error("âŒ [CRITICAL] WSS URL is missing from Secrets!")
            return

        self.is_running = True
        logger.info(f"ðŸš€ [ULTRA] Initializing {self.workers_count} Processing Workers...")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹Ù…Ø§Ù„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© (ÙƒØ§Ù…Ù„ Ù‚Ø¯Ø±Ø§ØªÙƒ Ù‡Ù†Ø§)
        workers = [asyncio.create_task(self._worker_logic(i)) for i in range(self.workers_count)]

        while self.is_running:
            try:
                # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø³Ø±Ø¹Ø© ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ
                async with websockets.connect(
                    self.wss_url, 
                    ping_interval=None, 
                    compression=None,   
                    extra_headers={"User-Agent": "Sovereign-Engine-v1.0"}
                ) as ws:
                    await self._subscribe(ws)
                    
                    while self.is_running:
                        raw_msg = await ws.recv()
                        if self._queue.full():
                            self._performance_metrics["dropped"] += 1
                            self._queue.get_nowait() 
                        
                        await self._queue.put((raw_msg, time.time()))

            except Exception as e:
                logger.error(f"âš ï¸ [CRITICAL] Radar Connection Lost: {e}")
                await asyncio.sleep(0.5) 

    async def _worker_logic(self, worker_id: int):
        """Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¹Ø§Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ: ØªØ­Ù„ÙŠÙ„ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© (Ù„Ù… ÙŠØªÙ… ØªØºÙŠÙŠØ± Ø­Ø±Ù ÙˆØ§Ø­Ø¯)"""
        while self.is_running:
            raw_msg, arrival_time = await self._queue.get()
            try:
                data = json.loads(raw_msg)
                if "params" in data:
                    result = data["params"]["result"]["value"]
                    event = self._deep_parse(result)
                    
                    if event:
                        # Ø£Ø±Ø´ÙØ© ÙÙˆØ±ÙŠØ© Ù…Ø¹ Ø­Ø³Ø§Ø¨ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ± (Latency)
                        latency = (time.time() - arrival_time) * 1000
                        await self.archiver.analyze_and_archive(
                            wallet=event.signature,
                            raw_data={"logs": event.raw_logs, "latency_ms": latency},
                            behavior_tag=event.event_type
                        )
                        self._performance_metrics["total_processed"] += 1
            except Exception as e:
                logger.error(f"Worker-{worker_id} Error: {e}")
            finally:
                self._queue.task_done()

    def _deep_parse(self, result: dict) -> Optional[MarketEvent]:
        """Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ: ÙØ­Øµ ÙƒØ«Ø§ÙØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Ù…Ù†Ø·Ù‚Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„ÙØ§Ø¦Ù‚)"""
        logs = result.get("logs", [])
        sig = result.get("signature")
        logs_str = "|".join(logs)

        if "mintTo" in logs_str and "InitializeMint" in logs_str:
            return MarketEvent(sig, time.time(), "INSTANT_BUNDLE_LAUNCH", 90, logs)
        
        if "SetAuthority" in logs_str and "Trade" in logs_str:
            return MarketEvent(sig, time.time(), "SAFE_DEV_ENTRY", 20, logs)

        if logs_str.count("Trade") > 10:
            return MarketEvent(sig, time.time(), "HIGH_FREQUENCY_ACCUMULATION", 60, logs)

        return None
